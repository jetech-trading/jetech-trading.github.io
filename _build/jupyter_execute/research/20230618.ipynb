{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dwj7d7z2oIPv"
   },
   "source": [
    "# 강화학습 트레이딩\n",
    "\n",
    "## 강화학습을 배우려는 이유\n",
    "\n",
    "트레이딩에 데이터를 이용하는 퀀트투자자들이 요즘 많이 생기고있습니다.\n",
    "하지만 계속 변화하는 환경에서 지속적으로 상관성이 낮은 알고리즘들을 모아가고싶었던 저는\n",
    "끊임없는 연구에 도움이 될 수 있는게 강화학습이라고 생각을해서 강화학습을 트레이딩에 적용하려고 노력중입니다.\n",
    "\n",
    "### 생물의 진화\n",
    "\n",
    "이 지구에 있는 생명체들, 각자 긴 세월을 보내오면서 진화를 거쳐왔고,\n",
    "우주의 급변하는 환경으로 생명체들에게는 적응과 생존이라는게 필요했습니다.\n",
    "\n",
    "생명체 고유의 특성으로 적응을 잘 하는 생명체들도 있겠으나.\n",
    "결국엔 기존과 다른, 작은 확률로 발현되는 돌연변이들이 적응과 생존에 유리하게되고\n",
    "그 개체가 결국 대표종이 되어 다음 세대를 이어갈 수 있습니다.\n",
    "\n",
    "물론 진화론을 믿지 않으시는 분들도 있지만,\n",
    "객관적으로 생각하는 습관, 논리적으로 따져보는 습관이 익숙한 저에게는\n",
    "트레이딩에서도 진화가 필요하다고 생각하고 있습니다.\n",
    "늘 같은 전략, 진리로 통하는 전략들도 있지만\n",
    "\n",
    "저의 목표인, 시장에 통하는 여러 알고리즘들을 찾아내기 위해선\n",
    "알고리즘들도 변화와 적응을 거쳐 진화해야한다고 생각하고 있습니다\n",
    "\n",
    "\n",
    "### 강화학습\n",
    "\n",
    "인공지능의 시대에 살고있는 우리들\n",
    "그중 강화학습은 다른 인공지능들과 어떤 차이점이 있을까요\n",
    "\n",
    "강화학습은 기존의 인공지능과 다른 차이점들이 있습니다.\n",
    "\n",
    "지도학습, 비지도학습과 차이점이 있긴하지만\n",
    "강화학습을 연구하다보면 결국 지도학습과 비지도학습의 개념이 강화학습이 포함되기도 합니다.\n",
    "\n",
    "우리가 가장 많이 알고있는 지도학습은\n",
    "우리가 정답을 알려주고 그대로 학습하고 패턴을 익히는 방법이라면\n",
    "강화학습은 보상주도 학습입니다.\n",
    "에이전트라는 개체를 두고 우리가 정의한 환경에서 상호작용하라고 구조를 짜두고\n",
    "이 짜여진판에서 어떤 행동을 하면 보상을 최대화 하는 방향을 얻을지 경험하게 됩니다.\n",
    "\n",
    "경험이 없는 상태에서는 임의로 행동을 하지만, exploration을 하면서 더 나은 행동에 대한 보상을 얻게됩니다. 최적의 행동임을 판단하고 이행하는것을 exploitation이라고 하는데 이 둘의 trade-off가 중요합니다. 우리가 늘 알던대로만 행동하면 성장하지 못하죠.\n",
    "\n",
    "변화하는 환경에서 돌연변이들이 적응하고 살아남은것들처럼\n",
    "진화에도 exploration이 존재했었습니다.\n",
    "이런 적은 확률의 exploration이 끝없이 변화하는 환경에서도 적응을 위한 씨앗이 될 것입니다.\n",
    "\n",
    "\n",
    "## 일단 적용해보기\n",
    "\n",
    "### Open AI\n",
    "\n",
    "Open AI는 인공지능 연구와 개발을 위한 오픈소스 플랫폼과 도구를 제공하는 연구소입니다.\n",
    "강화학습을 포함하여 다양한 인공지능 기술과 모델을 개발하고 배포하는데요\n",
    "대표적으로 GPT와 같은 언어모델을 개발하여 자연어 처리 분야에 큰 주목을 받고있습니다.\n",
    "\n",
    "### Open AI Gym\n",
    "\n",
    "Open AI Gym은 강화학습 알고리즘 개발을 위한 도구와 인터페이스를 제공하는 라이브러리입니다.\n",
    "이전에 강화학습은 우리가 만든 에이전트와 환경에서 상호작용을 통해 이루어진다고 하였죠.\n",
    "그런 에이전트와 환경을 제공하고, 커스터마이징할 수 있도록 제공합니다.\n",
    "\n",
    "\n",
    "### 트레이딩 환경\n",
    "\n",
    "강화학습의 원리를 보면\n",
    "에이전트와 환경의 상호작용과 상태, 보상 등 트레이딩에 존재하는 요소들을 담아낼 수 있겠다고 생각이 드실겁니다. gym에서도 trading환경에 대한 기본적인 환경을 제공하고 있습니다.\n",
    "오늘은 간단하게 이런 환경을 구성하고 에이전트가 상호작용할 수 있도록 구성해보려고 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6025,
     "status": "ok",
     "timestamp": 1687090849939,
     "user": {
      "displayName": "Jerry",
      "userId": "05552495483792684673"
     },
     "user_tz": -540
    },
    "id": "igPrtANbcwVd",
    "outputId": "0db2592e-8074-47bd-97ec-af4948a9aad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gym_anytrading in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Requirement already satisfied: gym>=0.12.5 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (1.22.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (3.7.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.12.5->gym_anytrading) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.12.5->gym_anytrading) (0.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->gym_anytrading) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->gym_anytrading) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->gym_anytrading) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (30, 2)\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "!pip install gym_anytrading\n",
    "import gym\n",
    "import gym_anytrading\n",
    "\n",
    "env = gym.make('stocks-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48nAlhaNctZr"
   },
   "source": [
    "\n",
    "\n",
    "gym, gym_anytrading을 import 합니다.\n",
    "\n",
    "trading, stocks 환경등이 포함되어있는데\n",
    "가장 기본적인 포맷이며 trading env에 비용, 수수료, 슬리피지 등의 추가 정보를 고려할 수 있습니다.\n",
    "\n",
    "하지만 어처피 모든것들을 나중에 커스터마이징 할 예정이고\n",
    "오늘은 간단히 환경을 소개하기 위해서 stocks-v0 환경을 다뤄보도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1687090932875,
     "user": {
      "displayName": "Jerry",
      "userId": "05552495483792684673"
     },
     "user_tz": -540
    },
    "id": "zdntSrJFcrNz",
    "outputId": "b89f410a-5633-4f2b-8d27-02b950342903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n",
      "(2335, 7)\n",
      "(2335,)\n",
      "(2335, 2)\n",
      "324533.23901761015\n",
      "1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "env.df['returns'] = env.df['Close'].pct_change()\n",
    "\n",
    "print(env.shape) # (30, 2)\n",
    "print(env.df.shape) # (2335, 6)\n",
    "print(env.prices.shape) # (2335, )\n",
    "print(env.signal_features.shape) # (2335, 2)\n",
    "print(env.max_possible_profit()) # 324533.23901761015\n",
    "print(len(env.df[env.df.returns > 0])) # 1227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gvzwdx0xcpXj"
   },
   "source": [
    "\n",
    "환경에 기본적으로 제공하는 데이터셋이 있는데\n",
    "\n",
    "구글의 일별 가격데이터를 제공하고 있습니다.\n",
    "\n",
    "env.shape를 보면 30행 2열의 데이터를 상태변수로 정의하고\n",
    "action은 0, 1이며 각각 매수하지 않고, 매수함을 의미합니다.\n",
    "\n",
    "여기서 30행, 2열의 데이터는\n",
    "30일의 데이터, 2열은 종가와 전일 일간 수익률입니다.\n",
    "\n",
    "> any_trading stocks-v0 env 구현 정보 : https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/stocks_env.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O3nJiBJhcHrU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1687091085206,
     "user": {
      "displayName": "Jerry",
      "userId": "05552495483792684673"
     },
     "user_tz": -540
    },
    "id": "GDZ6pEH9cloQ",
    "outputId": "c658ea6e-7cc1-49e8-d414-c4bf1840fde9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[202.982986,   0.600601],\n",
       "        [205.405411,   2.422425],\n",
       "        [208.823822,   3.418411],\n",
       "        [213.4935  ,   4.669678],\n",
       "        [214.414413,   0.920913],\n",
       "        [216.041046,   1.626633],\n",
       "        [220.360367,   4.319321],\n",
       "        [222.382385,   2.022018],\n",
       "        [219.604599,  -2.777786],\n",
       "        [218.02803 ,  -1.576569],\n",
       "        [216.51651 ,  -1.51152 ],\n",
       "        [214.714722,  -1.801788],\n",
       "        [212.632629,  -2.082093],\n",
       "        [208.593597,  -4.039032],\n",
       "        [208.208206,  -0.385391],\n",
       "        [207.787781,  -0.420425],\n",
       "        [207.237244,  -0.550537],\n",
       "        [210.255249,   3.018005],\n",
       "        [203.878876,  -6.376373],\n",
       "        [203.043045,  -0.835831],\n",
       "        [204.849854,   1.806809],\n",
       "        [208.093094,   3.24324 ],\n",
       "        [212.872879,   4.779785],\n",
       "        [212.282288,  -0.590591],\n",
       "        [211.006012,  -1.276276],\n",
       "        [209.704712,  -1.3013  ],\n",
       "        [204.449448,  -5.255264],\n",
       "        [205.01001 ,   0.560562],\n",
       "        [198.513519,  -6.496491],\n",
       "        [201.446442,   2.932923]]),\n",
       " 0,\n",
       " False,\n",
       " {'total_reward': 0.0, 'total_profit': 1.0, 'position': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQiq5KMKdxW4"
   },
   "source": [
    "env를 리셋하고 env에 대한 액션을 랜덤하게 주고, 해당 action을 env에가할경우\n",
    "\n",
    "순서대로 observation, reward, done, info를 전달받습니다.\n",
    "\n",
    "아래는 openai gym에 구현된 코드 내용입니다.\n",
    "\n",
    "```python\n",
    "    def step(self, action: ActType) -> Tuple[ObsType, float, bool, bool, dict]:\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "\n",
    "        When end of episode is reached, you are responsible for calling :meth:`reset` to reset this environment's state.\n",
    "        Accepts an action and returns either a tuple `(observation, reward, terminated, truncated, info)`.\n",
    "\n",
    "        Args:\n",
    "            action (ActType): an action provided by the agent\n",
    "\n",
    "        Returns:\n",
    "            observation (object): this will be an element of the environment's :attr:`observation_space`.\n",
    "                This may, for instance, be a numpy array containing the positions and velocities of certain objects.\n",
    "            reward (float): The amount of reward returned as a result of taking the action.\n",
    "            terminated (bool): whether a `terminal state` (as defined under the MDP of the task) is reached.\n",
    "                In this case further step() calls could return undefined results.\n",
    "            truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied.\n",
    "                Typically a timelimit, but could also be used to indicate agent physically going out of bounds.\n",
    "                Can be used to end the episode prematurely before a `terminal state` is reached.\n",
    "            info (dictionary): `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n",
    "                This might, for instance, contain: metrics that describe the agent's performance state, variables that are\n",
    "                hidden from observations, or individual reward terms that are combined to produce the total reward.\n",
    "                It also can contain information that distinguishes truncation and termination, however this is deprecated in favour\n",
    "                of returning two booleans, and will be removed in a future version.\n",
    "\n",
    "            (deprecated)\n",
    "            done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.\n",
    "                A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully,\n",
    "                a certain timelimit was exceeded, or the physics simulation has entered an invalid state.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "```\n",
    "\n",
    "open ai gym의 에서 truncated를 리턴하여 상호작용을 종료할 수 있는데\n",
    "\n",
    "any_trading stocks-v0환경에서는 이를 제외하고 done으로만 종료를 관리하도록 구현되어있습니다.\n",
    "\n",
    "그래서 총 4개의 변수를 반환받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXPCDlGGdz5K"
   },
   "source": [
    "## 에이전트의 학습 로직 구현\n",
    "\n",
    "환경을 구했으니 이제 에이전트가 필요합니다.\n",
    "\n",
    "일단 에이전트가 매번 동전을 던져서 앞면이면 매수하고\n",
    "\n",
    "뒷면이면 주식을 보유하지 않는 로직을 pseudo code로 구현해보았습니다.\n",
    "\n",
    "\n",
    "```python\n",
    "agent = new_agent()\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done: # 학습종료\n",
    "        break\n",
    "```\n",
    "\n",
    "이러면 우리는 에이전트라는것이 그저 운에 기반한 행위를 반복할 뿐이란걸 알 수 있습니다.\n",
    "\n",
    "여기서 필요한것들은 동전에 기반한 행동이 아닌, 주어진 상황을 보고 판단이라도 하게끔 인간의 두뇌역할을 할 무언갈 만들어주어야합니다.\n",
    "\n",
    "### 에이전트에게 두뇌 심어주기\n",
    "\n",
    "인간들은 인공지능의 능력을 보고 감탄하지만 사실 인간의 두뇌를 보고 만들어진 Deep-Neural-Network가 인공지능에서 핵심역할을 해내고있습니다.\n",
    "\n",
    "비록 강화학습에서도 이런 딥러닝 기술이 쓰이게 되는데요\n",
    "\n",
    "그저 동전만 던지는 에이전트에게 딥러닝 두뇌를 심어주도록 하겠습니다.\n",
    "\n",
    "인간도 주식시장 상황(가격)을 보고 판단(매수, 매도)을 하듯이\n",
    "\n",
    "에이전트에게도 상황(가격)을 보고 판단(매수, 매도)을 하는 딥러닝 모델을 심어주도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpfMsB6noLf2"
   },
   "source": [
    "## 에이전트에게 두뇌 만들어주기\n",
    "\n",
    "\n",
    "<img src=\"./rl_sample.jpg\" width=\"40%\" height=\"40%\" title=\"RL구성도\" alt=\"RL구성도\"></img>\n",
    "\n",
    "현재 우리가 가지고있는 에이전트는 동전을 던져서 나오는 결과와 같이 무작위 행동을 하고있습니다.\n",
    "\n",
    "조금 더 나은 행동을 하기 위해 우리는 두뇌를 만들어주는 역할을 하려고합니다.\n",
    "\n",
    "### 두뇌역할을 하는 DNN\n",
    "\n",
    "우리가 딥러닝에 사용하는 DNN레이어는 우리의 뇌 기능 구조를 따라 만들었습니다.\n",
    "\n",
    "두뇌는 수많은 뉴런으로 구성되어 있으며, 각 뉴런은 신경세포로서 정보를 처리하고 전달합니다. DNN도 비슷한 방식으로 여러 개의 인공 뉴런(노드)으로 구성되어 있으며, 이들은 입력을 받아 가중치와 활성화 함수를 통해 정보를 처리하고 출력을 생성합니다.\n",
    "\n",
    "두뇌와 DNN은 계층적인 구조를 가지고 있습니다. 두뇌는 다양한 수준의 계층으로 구성되어 있으며, 각 계층은 특정한 기능을 수행합니다. DNN도 입력 계층, 은닉 계층, 출력 계층 등으로 이루어진 계층 구조를 가지고 있습니다. 이러한 계층 구조를 통해 복잡한 문제를 단계적으로 해결할 수 있습니다.\n",
    "\n",
    "경험과 학습을 통해 성능을 향상시킬 수 있습니다. 두뇌는 경험과 지속적인 학습을 통해 새로운 정보를 인식하고, 지식을 쌓아나갑니다. 마찬가지로, DNN도 대량의 데이터를 학습하여 패턴을 파악하고 예측을 수행하는 능력을 키웁니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qhjpiKBD-qeu"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input: int,\n",
    "                 output: int,\n",
    "                 neurons: list = [64, 32],\n",
    "                 hidden: str = 'ReLU',\n",
    "                 out: str = 'Identity'):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.neurons = neurons\n",
    "        self.hidden = getattr(nn, hidden)()\n",
    "        self.out = getattr(nn, out)()\n",
    "\n",
    "        inputs = [input] + neurons\n",
    "        outputs = neurons + [output]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, (ins, outs) in enumerate(zip(inputs, outputs)):\n",
    "            is_last = True if i == len(inputs) - 1 else False\n",
    "            self.layers.append(nn.Linear(ins, outs)).to('cuda')\n",
    "            if is_last:\n",
    "                self.layers.append(self.outs).to('cuda')\n",
    "            else:\n",
    "                self.layers.append(self.hidden).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtLMTpWy-o26"
   },
   "source": [
    "\n",
    "### DNN을 이용한 강화학습 DQN\n",
    "\n",
    "\n",
    "<img src=\"./dnn_dqn.jpg\" width=\"40%\" height=\"40%\" title=\"dnn_dqn\" alt=\"dnn_dqn\"></img>\n",
    "\n",
    "\n",
    "DQN은 뇌의 신경망과 유사한 방식으로 작동합니다. DNN은 입력층, 은닉층, 출력층으로 구성된 계층적인 구조를 가지며, 각 층의 노드는 입력 데이터를 받아 가중치와 활성화 함수를 통해 정보를 처리합니다. DQN에서는 DNN이 Q-값 함수를 근사화하는 역할을 수행합니다.\n",
    "\n",
    "Q-값 함수는 주어진 상태에서 가능한 모든 행동에 대한 가치를 나타내는 함수로, 에이전트는 최대의 Q-값을 갖는 행동을 선택함으로써 최적의 행동을 결정합니다. DQN은 DNN을 사용하여 현재 상태를 입력으로 받아 Q-값을 예측하고, 이를 기반으로 행동을 선택하고 학습합니다. 학습은 실제 행동의 결과를 피드백으로 사용하여 DNN을 업데이트하고, 점진적으로 Q-값 함수를 개선하는 과정을 반복합니다.\n",
    "\n",
    "DQN은 딥러닝의 강력한 학습 능력을 활용하여 복잡한 환경에서도 효과적인 학습을 수행할 수 있습니다. 또한, DQN은 경험 재생(replay experience)과 타깃 네트워크(target network) 등의 기술을 적용하여 학습의 안정성과 효율성을 향상시킵니다.\n",
    "\n",
    "\n",
    "정리해보면 우리가 상태로 정의한 가격, 가격의 변화율을 보고 행동(매수, 매도)을 딥러닝을 통해 결정합니다. 사실 DQN의 핵심은 replay experience에 있지만 설명이 어려워질 수 있어서 생략하도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vWhdD7u-w7MI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state: int,\n",
    "                 action: int,\n",
    "                 qnet: nn.Module,\n",
    "                 qnet_target: nn.Module,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 eps: float):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.qnet = qnet\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
    "        self.register_buffer('epsilon', torch.ones(1) * eps)\n",
    "        self.qnet_target = qnet_target\n",
    "        self.criteria = nn.SmoothL1Loss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        qs = self.qnet(state)\n",
    "        prob = np.random.uniform(0.0, 1.0, 1)\n",
    "        if torch.from_numpy(prob).float() <= self.epsilon:  # epsilon 확률만큼은 random하게 exploration 할 수 있도록\n",
    "            action = np.random.choice(range(self.action_dim))\n",
    "        else:  # greedy\n",
    "            action = qs.argmax(dim=-1)\n",
    "        return int(action)\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        s, a, r, ns = state, action, reward, next_state\n",
    "\n",
    "        # compute Q-Learning target with 'target network'\n",
    "        with torch.no_grad():\n",
    "            q_max, _ = self.qnet_target(ns).max(dim=-1, keepdims=True)\n",
    "            q_target = r + self.gamma * q_max * (1 - done)\n",
    "\n",
    "        q_val = self.qnet(s).gather(1, a)\n",
    "        loss = self.criteria(q_val, q_target)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2npNoyu_uC5"
   },
   "source": [
    "### 에이전트 생성과 학습\n",
    "\n",
    "강화학습에 필요한 에이전트와 환경을 준비했으늬 이제 학습을 진행하도록 합니다.\n",
    "\n",
    "\n",
    "```python\n",
    "qnet = MLP(60, 2, num_neurons=[128])\n",
    "qnet_target = MLP(60, 2, num_neurons=[128])\n",
    "\n",
    "qnet_target.load_state_dict(qnet.state_dict())\n",
    "\n",
    "agent = DQN((30, 2), 2, qnet=qnet, qnet_target = qnet_target, lr=lr, gamma=gamma, epsilon=1.0)\n",
    "memory = ReplayMemory(memory_size)\n",
    "```\n",
    "\n",
    "qnet, qnet_target 두 가지를 둔 이유는,\n",
    "\n",
    "DQN은 주어진 상태에서 행동의 가치(Q-값)를 예측하고, 이를 기반으로 행동을 선택하여 학습합니다.\n",
    "\n",
    "그러나 이러한 학습 과정에서 신경망의 가중치가 지속적으로 변경되면서 예측과 선택이 서로 영향을 주고 받을 수 있습니다.\n",
    "\n",
    "이는 학습의 불안정성을 야기할 수 있습니다. 타깃 네트워크는 학습 과정에서 사용되는 신경망과 분리되어 있으므로, 예측과 선택 사이의 상호작용을 줄여 학습의 안정성을 높이는 데 도움을 줍니다.\n",
    "\n",
    "타깃 네트워크는 일정한 주기로 학습 중인 신경망의 가중치로 업데이트됩니다.\n",
    "\n",
    "이는 학습 중에 현재 상태의 가치를 기준으로 행동을 선택하지 않고, 이전 가중치를 사용하여 Q-값을 추정함으로써 목표 업데이트를 지연시킵니다.\n",
    "\n",
    "이를 통해 학습이 안정화되고, 특정 상태에서의 편향된 예측을 방지할 수 있습니다.\n",
    "\n",
    "\n",
    "```python\n",
    "for n_epi in range(total_eps):\n",
    "    # epsilon = eps_max = 0.01\n",
    "    epsilon = max(eps_min, eps_max - eps_min * (n_epi / 200))\n",
    "    agent.epsilon = torch.tensor(epsilon).to('cuda')\n",
    "    s = env.reset()\n",
    "    cum_r = 0\n",
    "\n",
    "    while True:\n",
    "        s = to_tensor(s, size=(30, 2))\n",
    "        s = s.reshape(1, 60).to('cuda')\n",
    "        a = agent.get_action(s)\n",
    "\n",
    "        ns, r, done, info = env.step(a)\n",
    "\n",
    "        experience = (s,\n",
    "                    torch.tensor(a).view(1, 1).to('cuda'),\n",
    "                    torch.tensor(r).view(1, 1).to('cuda'),\n",
    "                    torch.tensor(ns).view(30, 2).reshape(1, 60).to('cuda'),\n",
    "                    torch.tensor(done).view(1, 1).to('cuda'))\n",
    "        memory.push(experience)\n",
    "\n",
    "        s = ns\n",
    "        cum_r += r\n",
    "        if done:\n",
    "            # print('info: ', info)\n",
    "            break\n",
    "\n",
    "        if len(memory) >= sampling_only_until:\n",
    "            sampled_exps = memory.sample(batch_size)\n",
    "            sampled_exps = prepare_training_inputs(sampled_exps)\n",
    "            agent.update(*sampled_exps)\n",
    "\n",
    "    if n_epi % target_update_interval == 0:\n",
    "        agent.qnet_target.load_state_dict(agent.qnet.state_dict())\n",
    "    if n_epi % print_every == 0:\n",
    "        msg = (n_epi, cum_r, epsilon)\n",
    "        agent_monitor_df = pd.concat([agent_monitor_df, pd.DataFrame([[n_epi, cum_r, epsilon]], columns=agent_monitor_df.columns)])\n",
    "        print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))\n",
    "\n",
    "```\n",
    "\n",
    "agent가 행동을 취할때 exploration을 적절히 할 수 있도록 설정했었는데\n",
    "\n",
    "초기에는 epsilon을 크게잡고, 에피소드가 진행되도록 줄여나가는 decaying epsilon 로직이 적용되어있습니다.\n",
    "\n",
    "그리고 memory buffer에 담긴 내용을 하나씩 꺼내와서 agent에게 주기적으로 랜덤한 데이터를 학습시킵니다. 모델이 최신상태만 보며 과적합되지 않고 수집해왔던 과거의 데이터들도 살펴보면서 학습을 진행합니다.\n",
    "\n",
    "\n",
    "<img src=\"./rl_train.jpg\" width=\"40%\" height=\"40%\" title=\"rl_train\" alt=\"rl_train\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivqghRNpd3v9"
   },
   "source": [
    "## 느낀점과 최신동향\n",
    "\n",
    "강화학습은 환경과 상호작용하며 얻는 상태를 어떻게 정의하고\n",
    "보상을 어떻게 정의하느냐에 따라서 무궁무진한 시나리오가 존재하고 에이전트의 행동등 고려할 것들이 너무 많은 고차원 영역임에 쉽지 않습니다.\n",
    "\n",
    "강화학습을 위한 환경, 에이전트, 상태, 행동과 보상을 기획하는건 도메인 지식이 필요하다는 것이겠죠. 우리가 단순히 종가만을 다루고 있지만, 드레이딩에서 종가데이터가 가지는 의미에 대해서 알고있고, 블록체인 트레이딩이라면 블록체인 layer2에 tvl과 코인, 토큰들의 상관관계와 정성적으론 G2국가가 블록체인 사업에 대한 제재들도 이해하고있어야함을 의미합니다. 모든 쉬운게 없지만 제가 관심있는 분야이기 때문에 힘을내서 연구할 수 있는것같습니다.\n",
    "\n",
    "회사에서 ai 교육을 받을 일이 있어서 서울대 데이터사이언스 대학원 교수님과 박사과정분들의 논문리뷰를 듣고왔는데\n",
    "\n",
    "최신 강화학습의 노력들을 보면 수학적으로 에이전트가 행동과 보상의 한계선, 그 한계선들을 조금이라도 늘리는 싸움을 하고있음을 느꼈습니다.\n",
    "\n",
    "그리고 정작 실제 데이터에 적용하기 위해선 제약사항도 아직 너무 많아서 마치 초창기 우주와 같이 우리가 모르는것들도 계속해서 나오고있고 새로운 알고리즘들과 패러다임이 등장하는 현재진행중인 학문인게 느껴졌습니다.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}