

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>강화학습 트레이딩 &#8212; Trading Research</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'research/20230618';</script>
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="소개" href="research.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    소개
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Trading</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../stock/stock.html">Stock</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../stock/20230606.html">2023 5월 주식 데이터 리뷰</a></li>
<li class="toctree-l2"><a class="reference internal" href="../markdown-notebooks.html">Notebooks with MyST Markdown</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../crypto/crypto.html">Crypto</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../crypto/20230606.html">2023.06.06 market test</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="research.html">소개</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">강화학습 트레이딩</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/research/20230618.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>강화학습 트레이딩</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">강화학습을 배우려는 이유</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">생물의 진화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">강화학습</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">일단 적용해보기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-ai">Open AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-ai-gym">Open AI Gym</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">트레이딩 환경</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">에이전트의 학습 로직 구현</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">에이전트에게 두뇌 심어주기</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">에이전트에게 두뇌 만들어주기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn">두뇌역할을 하는 DNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-dqn">DNN을 이용한 강화학습 DQN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">에이전트 생성과 학습</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">느낀점과 최신동향</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>강화학습 트레이딩<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h1>
<div class="section" id="id2">
<h2>강화학습을 배우려는 이유<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>트레이딩에 데이터를 이용하는 퀀트투자자들이 요즘 많이 생기고있습니다.
하지만 계속 변화하는 환경에서 지속적으로 상관성이 낮은 알고리즘들을 모아가고싶었던 저는
끊임없는 연구에 도움이 될 수 있는게 강화학습이라고 생각을해서 강화학습을 트레이딩에 적용하려고 노력중입니다.</p>
<div class="section" id="id3">
<h3>생물의 진화<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>이 지구에 있는 생명체들, 각자 긴 세월을 보내오면서 진화를 거쳐왔고,
우주의 급변하는 환경으로 생명체들에게는 적응과 생존이라는게 필요했습니다.</p>
<p>생명체 고유의 특성으로 적응을 잘 하는 생명체들도 있겠으나.
결국엔 기존과 다른, 작은 확률로 발현되는 돌연변이들이 적응과 생존에 유리하게되고
그 개체가 결국 대표종이 되어 다음 세대를 이어갈 수 있습니다.</p>
<p>물론 진화론을 믿지 않으시는 분들도 있지만,
객관적으로 생각하는 습관, 논리적으로 따져보는 습관이 익숙한 저에게는
트레이딩에서도 진화가 필요하다고 생각하고 있습니다.
늘 같은 전략, 진리로 통하는 전략들도 있지만</p>
<p>저의 목표인, 시장에 통하는 여러 알고리즘들을 찾아내기 위해선
알고리즘들도 변화와 적응을 거쳐 진화해야한다고 생각하고 있습니다</p>
</div>
<div class="section" id="id4">
<h3>강화학습<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>인공지능의 시대에 살고있는 우리들
그중 강화학습은 다른 인공지능들과 어떤 차이점이 있을까요</p>
<p>강화학습은 기존의 인공지능과 다른 차이점들이 있습니다.</p>
<p>지도학습, 비지도학습과 차이점이 있긴하지만
강화학습을 연구하다보면 결국 지도학습과 비지도학습의 개념이 강화학습이 포함되기도 합니다.</p>
<p>우리가 가장 많이 알고있는 지도학습은
우리가 정답을 알려주고 그대로 학습하고 패턴을 익히는 방법이라면
강화학습은 보상주도 학습입니다.
에이전트라는 개체를 두고 우리가 정의한 환경에서 상호작용하라고 구조를 짜두고
이 짜여진판에서 어떤 행동을 하면 보상을 최대화 하는 방향을 얻을지 경험하게 됩니다.</p>
<p>경험이 없는 상태에서는 임의로 행동을 하지만, exploration을 하면서 더 나은 행동에 대한 보상을 얻게됩니다. 최적의 행동임을 판단하고 이행하는것을 exploitation이라고 하는데 이 둘의 trade-off가 중요합니다. 우리가 늘 알던대로만 행동하면 성장하지 못하죠.</p>
<p>변화하는 환경에서 돌연변이들이 적응하고 살아남은것들처럼
진화에도 exploration이 존재했었습니다.
이런 적은 확률의 exploration이 끝없이 변화하는 환경에서도 적응을 위한 씨앗이 될 것입니다.</p>
</div>
</div>
<div class="section" id="id5">
<h2>일단 적용해보기<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h2>
<div class="section" id="open-ai">
<h3>Open AI<a class="headerlink" href="#open-ai" title="Permalink to this heading">#</a></h3>
<p>Open AI는 인공지능 연구와 개발을 위한 오픈소스 플랫폼과 도구를 제공하는 연구소입니다.
강화학습을 포함하여 다양한 인공지능 기술과 모델을 개발하고 배포하는데요
대표적으로 GPT와 같은 언어모델을 개발하여 자연어 처리 분야에 큰 주목을 받고있습니다.</p>
</div>
<div class="section" id="open-ai-gym">
<h3>Open AI Gym<a class="headerlink" href="#open-ai-gym" title="Permalink to this heading">#</a></h3>
<p>Open AI Gym은 강화학습 알고리즘 개발을 위한 도구와 인터페이스를 제공하는 라이브러리입니다.
이전에 강화학습은 우리가 만든 에이전트와 환경에서 상호작용을 통해 이루어진다고 하였죠.
그런 에이전트와 환경을 제공하고, 커스터마이징할 수 있도록 제공합니다.</p>
</div>
<div class="section" id="id6">
<h3>트레이딩 환경<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p>강화학습의 원리를 보면
에이전트와 환경의 상호작용과 상태, 보상 등 트레이딩에 존재하는 요소들을 담아낼 수 있겠다고 생각이 드실겁니다. gym에서도 trading환경에 대한 기본적인 환경을 제공하고 있습니다.
오늘은 간단하게 이런 환경을 구성하고 에이전트가 상호작용할 수 있도록 구성해보려고 합니다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>gym_anytrading
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">gym_anytrading</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;stocks-v0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
Requirement already satisfied: gym_anytrading in /usr/local/lib/python3.10/dist-packages (1.3.2)
Requirement already satisfied: gym&gt;=0.12.5 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (0.25.2)
Requirement already satisfied: numpy&gt;=1.16.4 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (1.22.4)
Requirement already satisfied: pandas&gt;=0.24.2 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (1.5.3)
Requirement already satisfied: matplotlib&gt;=3.1.1 in /usr/local/lib/python3.10/dist-packages (from gym_anytrading) (3.7.1)
Requirement already satisfied: cloudpickle&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym&gt;=0.12.5-&gt;gym_anytrading) (2.2.1)
Requirement already satisfied: gym-notices&gt;=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym&gt;=0.12.5-&gt;gym_anytrading) (0.0.8)
Requirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (1.0.7)
Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (4.39.3)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (1.4.4)
Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (23.1)
Requirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (8.4.0)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (3.0.9)
Requirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.1.1-&gt;gym_anytrading) (2.8.2)
Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=0.24.2-&gt;gym_anytrading) (2022.7.1)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.1.1-&gt;gym_anytrading) (1.16.0)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:31: UserWarning: <span class=" -Color -Color-Yellow">WARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (30, 2)</span>
  logger.warn(
/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: <span class=" -Color -Color-Yellow">WARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.</span>
  deprecation(
/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: <span class=" -Color -Color-Yellow">WARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.</span>
  deprecation(
</pre></div>
</div>
</div>
</div>
<p>gym, gym_anytrading을 import 합니다.</p>
<p>trading, stocks 환경등이 포함되어있는데
가장 기본적인 포맷이며 trading env에 비용, 수수료, 슬리피지 등의 추가 정보를 고려할 수 있습니다.</p>
<p>하지만 어처피 모든것들을 나중에 커스터마이징 할 예정이고
오늘은 간단히 환경을 소개하기 위해서 stocks-v0 환경을 다뤄보도록 하겠습니다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;returns&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Close&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">pct_change</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (30, 2)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (2335, 6)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">prices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (2335, )</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">signal_features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (2335, 2)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">max_possible_profit</span><span class="p">())</span> <span class="c1"># 324533.23901761015</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">df</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">returns</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]))</span> <span class="c1"># 1227</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(30, 2)
(2335, 7)
(2335,)
(2335, 2)
324533.23901761015
1227
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.
  and should_run_async(code)
</pre></div>
</div>
</div>
</div>
<p>환경에 기본적으로 제공하는 데이터셋이 있는데</p>
<p>구글의 일별 가격데이터를 제공하고 있습니다.</p>
<p>env.shape를 보면 30행 2열의 데이터를 상태변수로 정의하고
action은 0, 1이며 각각 매수하지 않고, 매수함을 의미합니다.</p>
<p>여기서 30행, 2열의 데이터는
30일의 데이터, 2열은 종가와 전일 일간 수익률입니다.</p>
<blockquote>
<div><p>any_trading stocks-v0 env 구현 정보 : <a class="github reference external" href="https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/envs/stocks_env.py">AminHP/gym-anytrading</a></p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.
  and should_run_async(code)
/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: <span class=" -Color -Color-Yellow">WARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.</span>
  logger.warn(
/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: <span class=" -Color -Color-Yellow">WARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.</span>
  logger.warn(
/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: <span class=" -Color -Color-Yellow">WARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.</span>
  logger.warn(
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: <span class=" -Color -Color-Yellow">WARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. </span>
  logger.deprecation(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[202.982986,   0.600601],
        [205.405411,   2.422425],
        [208.823822,   3.418411],
        [213.4935  ,   4.669678],
        [214.414413,   0.920913],
        [216.041046,   1.626633],
        [220.360367,   4.319321],
        [222.382385,   2.022018],
        [219.604599,  -2.777786],
        [218.02803 ,  -1.576569],
        [216.51651 ,  -1.51152 ],
        [214.714722,  -1.801788],
        [212.632629,  -2.082093],
        [208.593597,  -4.039032],
        [208.208206,  -0.385391],
        [207.787781,  -0.420425],
        [207.237244,  -0.550537],
        [210.255249,   3.018005],
        [203.878876,  -6.376373],
        [203.043045,  -0.835831],
        [204.849854,   1.806809],
        [208.093094,   3.24324 ],
        [212.872879,   4.779785],
        [212.282288,  -0.590591],
        [211.006012,  -1.276276],
        [209.704712,  -1.3013  ],
        [204.449448,  -5.255264],
        [205.01001 ,   0.560562],
        [198.513519,  -6.496491],
        [201.446442,   2.932923]]),
 0,
 False,
 {&#39;total_reward&#39;: 0.0, &#39;total_profit&#39;: 1.0, &#39;position&#39;: 1})
</pre></div>
</div>
</div>
</div>
<p>env를 리셋하고 env에 대한 액션을 랜덤하게 주고, 해당 action을 env에가할경우</p>
<p>순서대로 observation, reward, done, info를 전달받습니다.</p>
<p>아래는 openai gym에 구현된 코드 내용입니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="n">ActType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ObsType</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Run one timestep of the environment&#39;s dynamics.</span>

<span class="sd">        When end of episode is reached, you are responsible for calling :meth:`reset` to reset this environment&#39;s state.</span>
<span class="sd">        Accepts an action and returns either a tuple `(observation, reward, terminated, truncated, info)`.</span>

<span class="sd">        Args:</span>
<span class="sd">            action (ActType): an action provided by the agent</span>

<span class="sd">        Returns:</span>
<span class="sd">            observation (object): this will be an element of the environment&#39;s :attr:`observation_space`.</span>
<span class="sd">                This may, for instance, be a numpy array containing the positions and velocities of certain objects.</span>
<span class="sd">            reward (float): The amount of reward returned as a result of taking the action.</span>
<span class="sd">            terminated (bool): whether a `terminal state` (as defined under the MDP of the task) is reached.</span>
<span class="sd">                In this case further step() calls could return undefined results.</span>
<span class="sd">            truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied.</span>
<span class="sd">                Typically a timelimit, but could also be used to indicate agent physically going out of bounds.</span>
<span class="sd">                Can be used to end the episode prematurely before a `terminal state` is reached.</span>
<span class="sd">            info (dictionary): `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).</span>
<span class="sd">                This might, for instance, contain: metrics that describe the agent&#39;s performance state, variables that are</span>
<span class="sd">                hidden from observations, or individual reward terms that are combined to produce the total reward.</span>
<span class="sd">                It also can contain information that distinguishes truncation and termination, however this is deprecated in favour</span>
<span class="sd">                of returning two booleans, and will be removed in a future version.</span>

<span class="sd">            (deprecated)</span>
<span class="sd">            done (bool): A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.</span>
<span class="sd">                A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully,</span>
<span class="sd">                a certain timelimit was exceeded, or the physics simulation has entered an invalid state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

</pre></div>
</div>
<p>open ai gym의 에서 truncated를 리턴하여 상호작용을 종료할 수 있는데</p>
<p>any_trading stocks-v0환경에서는 이를 제외하고 done으로만 종료를 관리하도록 구현되어있습니다.</p>
<p>그래서 총 4개의 변수를 반환받습니다.</p>
</div>
</div>
<div class="section" id="id7">
<h2>에이전트의 학습 로직 구현<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h2>
<p>환경을 구했으니 이제 에이전트가 필요합니다.</p>
<p>일단 에이전트가 매번 동전을 던져서 앞면이면 매수하고</p>
<p>뒷면이면 주식을 보유하지 않는 로직을 pseudo code로 구현해보았습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">new_agent</span><span class="p">()</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="c1"># 학습종료</span>
        <span class="k">break</span>
</pre></div>
</div>
<p>이러면 우리는 에이전트라는것이 그저 운에 기반한 행위를 반복할 뿐이란걸 알 수 있습니다.</p>
<p>여기서 필요한것들은 동전에 기반한 행동이 아닌, 주어진 상황을 보고 판단이라도 하게끔 인간의 두뇌역할을 할 무언갈 만들어주어야합니다.</p>
<div class="section" id="id8">
<h3>에이전트에게 두뇌 심어주기<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<p>인간들은 인공지능의 능력을 보고 감탄하지만 사실 인간의 두뇌를 보고 만들어진 Deep-Neural-Network가 인공지능에서 핵심역할을 해내고있습니다.</p>
<p>비록 강화학습에서도 이런 딥러닝 기술이 쓰이게 되는데요</p>
<p>그저 동전만 던지는 에이전트에게 딥러닝 두뇌를 심어주도록 하겠습니다.</p>
<p>인간도 주식시장 상황(가격)을 보고 판단(매수, 매도)을 하듯이</p>
<p>에이전트에게도 상황(가격)을 보고 판단(매수, 매도)을 하는 딥러닝 모델을 심어주도록 하겠습니다.</p>
</div>
</div>
<div class="section" id="id9">
<h2>에이전트에게 두뇌 만들어주기<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h2>
<p><img src="./rl_sample.jpg" width="40%" height="40%" title="RL구성도" alt="RL구성도"></img></p>
<p>현재 우리가 가지고있는 에이전트는 동전을 던져서 나오는 결과와 같이 무작위 행동을 하고있습니다.</p>
<p>조금 더 나은 행동을 하기 위해 우리는 두뇌를 만들어주는 역할을 하려고합니다.</p>
<div class="section" id="dnn">
<h3>두뇌역할을 하는 DNN<a class="headerlink" href="#dnn" title="Permalink to this heading">#</a></h3>
<p>우리가 딥러닝에 사용하는 DNN레이어는 우리의 뇌 기능 구조를 따라 만들었습니다.</p>
<p>두뇌는 수많은 뉴런으로 구성되어 있으며, 각 뉴런은 신경세포로서 정보를 처리하고 전달합니다. DNN도 비슷한 방식으로 여러 개의 인공 뉴런(노드)으로 구성되어 있으며, 이들은 입력을 받아 가중치와 활성화 함수를 통해 정보를 처리하고 출력을 생성합니다.</p>
<p>두뇌와 DNN은 계층적인 구조를 가지고 있습니다. 두뇌는 다양한 수준의 계층으로 구성되어 있으며, 각 계층은 특정한 기능을 수행합니다. DNN도 입력 계층, 은닉 계층, 출력 계층 등으로 이루어진 계층 구조를 가지고 있습니다. 이러한 계층 구조를 통해 복잡한 문제를 단계적으로 해결할 수 있습니다.</p>
<p>경험과 학습을 통해 성능을 향상시킬 수 있습니다. 두뇌는 경험과 지속적인 학습을 통해 새로운 정보를 인식하고, 지식을 쌓아나갑니다. 마찬가지로, DNN도 대량의 데이터를 학습하여 패턴을 파악하고 예측을 수행하는 능력을 키웁니다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="nb">input</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">output</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">neurons</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
                 <span class="n">hidden</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">,</span>
                 <span class="n">out</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Identity&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neurons</span> <span class="o">=</span> <span class="n">neurons</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">out</span><span class="p">)()</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span> <span class="o">+</span> <span class="n">neurons</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">neurons</span> <span class="o">+</span> <span class="p">[</span><span class="n">output</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ins</span><span class="p">,</span> <span class="n">outs</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ins</span><span class="p">,</span> <span class="n">outs</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_last</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outs</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="dnn-dqn">
<h3>DNN을 이용한 강화학습 DQN<a class="headerlink" href="#dnn-dqn" title="Permalink to this heading">#</a></h3>
<p><img src="./dnn_dqn.jpg" width="40%" height="40%" title="dnn_dqn" alt="dnn_dqn"></img></p>
<p>DQN은 뇌의 신경망과 유사한 방식으로 작동합니다. DNN은 입력층, 은닉층, 출력층으로 구성된 계층적인 구조를 가지며, 각 층의 노드는 입력 데이터를 받아 가중치와 활성화 함수를 통해 정보를 처리합니다. DQN에서는 DNN이 Q-값 함수를 근사화하는 역할을 수행합니다.</p>
<p>Q-값 함수는 주어진 상태에서 가능한 모든 행동에 대한 가치를 나타내는 함수로, 에이전트는 최대의 Q-값을 갖는 행동을 선택함으로써 최적의 행동을 결정합니다. DQN은 DNN을 사용하여 현재 상태를 입력으로 받아 Q-값을 예측하고, 이를 기반으로 행동을 선택하고 학습합니다. 학습은 실제 행동의 결과를 피드백으로 사용하여 DNN을 업데이트하고, 점진적으로 Q-값 함수를 개선하는 과정을 반복합니다.</p>
<p>DQN은 딥러닝의 강력한 학습 능력을 활용하여 복잡한 환경에서도 효과적인 학습을 수행할 수 있습니다. 또한, DQN은 경험 재생(replay experience)과 타깃 네트워크(target network) 등의 기술을 적용하여 학습의 안정성과 효율성을 향상시킵니다.</p>
<p>정리해보면 우리가 상태로 정의한 가격, 가격의 변화율을 보고 행동(매수, 매도)을 딥러닝을 통해 결정합니다. 사실 DQN의 핵심은 replay experience에 있지만 설명이 어려워질 수 있어서 생략하도록 하겠습니다.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">state</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">qnet</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                 <span class="n">qnet_target</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                 <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DQN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="n">action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qnet</span> <span class="o">=</span> <span class="n">qnet</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">qnet</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qnet_target</span> <span class="o">=</span> <span class="n">qnet_target</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">qs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qnet</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>  <span class="c1"># epsilon 확률만큼은 random하게 exploration 할 수 있도록</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># greedy</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">qs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span> <span class="o">=</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span>

        <span class="c1"># compute Q-Learning target with &#39;target network&#39;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">q_max</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qnet_target</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">q_target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">q_max</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span>

        <span class="n">q_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qnet</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criteria</span><span class="p">(</span><span class="n">q_val</span><span class="p">,</span> <span class="n">q_target</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.
  and should_run_async(code)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id10">
<h3>에이전트 생성과 학습<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h3>
<p>강화학습에 필요한 에이전트와 환경을 준비했으늬 이제 학습을 진행하도록 합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">qnet</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">])</span>
<span class="n">qnet_target</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_neurons</span><span class="o">=</span><span class="p">[</span><span class="mi">128</span><span class="p">])</span>

<span class="n">qnet_target</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">qnet</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">qnet</span><span class="o">=</span><span class="n">qnet</span><span class="p">,</span> <span class="n">qnet_target</span> <span class="o">=</span> <span class="n">qnet_target</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">ReplayMemory</span><span class="p">(</span><span class="n">memory_size</span><span class="p">)</span>
</pre></div>
</div>
<p>qnet, qnet_target 두 가지를 둔 이유는,</p>
<p>DQN은 주어진 상태에서 행동의 가치(Q-값)를 예측하고, 이를 기반으로 행동을 선택하여 학습합니다.</p>
<p>그러나 이러한 학습 과정에서 신경망의 가중치가 지속적으로 변경되면서 예측과 선택이 서로 영향을 주고 받을 수 있습니다.</p>
<p>이는 학습의 불안정성을 야기할 수 있습니다. 타깃 네트워크는 학습 과정에서 사용되는 신경망과 분리되어 있으므로, 예측과 선택 사이의 상호작용을 줄여 학습의 안정성을 높이는 데 도움을 줍니다.</p>
<p>타깃 네트워크는 일정한 주기로 학습 중인 신경망의 가중치로 업데이트됩니다.</p>
<p>이는 학습 중에 현재 상태의 가치를 기준으로 행동을 선택하지 않고, 이전 가중치를 사용하여 Q-값을 추정함으로써 목표 업데이트를 지연시킵니다.</p>
<p>이를 통해 학습이 안정화되고, 특정 상태에서의 편향된 예측을 방지할 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n_epi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_eps</span><span class="p">):</span>
    <span class="c1"># epsilon = eps_max = 0.01</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">eps_min</span><span class="p">,</span> <span class="n">eps_max</span> <span class="o">-</span> <span class="n">eps_min</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_epi</span> <span class="o">/</span> <span class="mi">200</span><span class="p">))</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">cum_r</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="n">ns</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="n">experience</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">),</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">done</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">))</span>
        <span class="n">memory</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">ns</span>
        <span class="n">cum_r</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># print(&#39;info: &#39;, info)</span>
            <span class="k">break</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">sampling_only_until</span><span class="p">:</span>
            <span class="n">sampled_exps</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">sampled_exps</span> <span class="o">=</span> <span class="n">prepare_training_inputs</span><span class="p">(</span><span class="n">sampled_exps</span><span class="p">)</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="o">*</span><span class="n">sampled_exps</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">n_epi</span> <span class="o">%</span> <span class="n">target_update_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">qnet_target</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">qnet</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">n_epi</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_epi</span><span class="p">,</span> <span class="n">cum_r</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="n">agent_monitor_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">agent_monitor_df</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="n">n_epi</span><span class="p">,</span> <span class="n">cum_r</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="n">agent_monitor_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Episode : </span><span class="si">{:4.0f}</span><span class="s2"> | Cumulative Reward : </span><span class="si">{:4.0f}</span><span class="s2"> | Epsilon : </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">msg</span><span class="p">))</span>

</pre></div>
</div>
<p>agent가 행동을 취할때 exploration을 적절히 할 수 있도록 설정했었는데</p>
<p>초기에는 epsilon을 크게잡고, 에피소드가 진행되도록 줄여나가는 decaying epsilon 로직이 적용되어있습니다.</p>
<p>그리고 memory buffer에 담긴 내용을 하나씩 꺼내와서 agent에게 주기적으로 랜덤한 데이터를 학습시킵니다. 모델이 최신상태만 보며 과적합되지 않고 수집해왔던 과거의 데이터들도 살펴보면서 학습을 진행합니다.</p>
<p><img src="./rl_train.jpg" width="40%" height="40%" title="rl_train" alt="rl_train"></img></p>
</div>
</div>
<div class="section" id="id11">
<h2>느낀점과 최신동향<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h2>
<p>강화학습은 환경과 상호작용하며 얻는 상태를 어떻게 정의하고
보상을 어떻게 정의하느냐에 따라서 무궁무진한 시나리오가 존재하고 에이전트의 행동등 고려할 것들이 너무 많은 고차원 영역임에 쉽지 않습니다.</p>
<p>강화학습을 위한 환경, 에이전트, 상태, 행동과 보상을 기획하는건 도메인 지식이 필요하다는 것이겠죠. 우리가 단순히 종가만을 다루고 있지만, 드레이딩에서 종가데이터가 가지는 의미에 대해서 알고있고, 블록체인 트레이딩이라면 블록체인 layer2에 tvl과 코인, 토큰들의 상관관계와 정성적으론 G2국가가 블록체인 사업에 대한 제재들도 이해하고있어야함을 의미합니다. 모든 쉬운게 없지만 제가 관심있는 분야이기 때문에 힘을내서 연구할 수 있는것같습니다.</p>
<p>회사에서 ai 교육을 받을 일이 있어서 서울대 데이터사이언스 대학원 교수님과 박사과정분들의 논문리뷰를 듣고왔는데</p>
<p>최신 강화학습의 노력들을 보면 수학적으로 에이전트가 행동과 보상의 한계선, 그 한계선들을 조금이라도 늘리는 싸움을 하고있음을 느꼈습니다.</p>
<p>그리고 정작 실제 데이터에 적용하기 위해선 제약사항도 아직 너무 많아서 마치 초창기 우주와 같이 우리가 모르는것들도 계속해서 나오고있고 새로운 알고리즘들과 패러다임이 등장하는 현재진행중인 학문인게 느껴졌습니다.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./research"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="research.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">소개</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">강화학습을 배우려는 이유</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">생물의 진화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">강화학습</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">일단 적용해보기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-ai">Open AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-ai-gym">Open AI Gym</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">트레이딩 환경</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">에이전트의 학습 로직 구현</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">에이전트에게 두뇌 심어주기</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">에이전트에게 두뇌 만들어주기</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn">두뇌역할을 하는 DNN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dnn-dqn">DNN을 이용한 강화학습 DQN</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">에이전트 생성과 학습</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">느낀점과 최신동향</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By JeTech
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>